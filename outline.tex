\documentclass[12pt]{article}
\usepackage[breaklinks=true]{hyperref}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
\usepackage[margin=0.75in]{geometry}
\usepackage[singlespacing]{setspace}
\usepackage[bottom]{footmisc}
\usepackage{floatrow}
\usepackage{float,graphicx}
\usepackage{enumerate}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}{\mathbb{I}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution 
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}

\title{Outline: PRNGs and Permutations}
\author{Kellie Ottoboni}
\date{Draft \today}
\begin{document}
\maketitle

%\newpage

%\begin{abstract}


%\end{abstract}

%\newpage

\section{Standard Implementations}
 The first order of business is to investigate how R and Python generate pseudo-random numbers and
what algorithms they use to sample, permute, shuffle, etc.
This may requires looking at the source code.
\begin{itemize}
\item Python, R, SAS, SPSS, and Matlab use the Mersenne Twister as their default PRNG.
Stata uses the KISS PRNG.
Excel 2003 used the Wichmann-Hill generator, but implemented it incorrectly; Excel 2007 attempted to fix the bug but failed (McCullough 2008).
Now, Excel uses the Mersenne Twister.
\item There doesn't seem to be any way to see what \texttt{sample} does directly.
However, someone has implemented a C++ version for RcppArmadillo at \url{https://github.com/RcppCore/RcppArmadillo/blob/master/inst/include/RcppArmadilloExtensions/sample.h}.
Someone else has put the R base C source code in an unaffiliated GitHub repository at \url{https://github.com/SurajGupta/r-source/blob/master/src/main/random.c}.
Here, we see that R's \texttt{sample} is broken down into four separate functions.w
The functions sample by first randomly selecting indices for the new sample, then copying the original data into the return vector using the new indices.
\begin{enumerate}
\item \textbf{Equal probability sampling with replacement:} 
for each of $k$ samples, generate a uniform PRN;
multiply it by the population size $n$ and add one, then take the floor function.
This also requires $k$ uniform PRNs.
\item \textbf{Equal probability sampling without replacement:} 
Start with a population of size $n$.
For each of $k$ samples, generate a uniform PRN;
multiply it by the current population size $\tilde{n}$ and take the floor, then add one -- this is the selected index;
decrement the current population size $\tilde{n}$ by one and put the last remaining index in the place of the selected index.
This only requires $k$ uniform PRNs.
\item \textbf{Unequal probability sampling with replacement:} 
Sort sampling probabilities and the corresponding indices in descending order; 
compute cumulative probabilities; 
for each of $n$ samples, generate a random uniform and select the index $j$ such that the random number is less than or equal to the $j$th cumulative probability but not the $j+1$st.  
This requires $n$ uniform PRNs.  
OR Walker's alias method for large samples.
\item \textbf{Unequal probability sampling without replacement:} 
Sort sampling probabilities and the corresponding indices in descending order; 
for each of $n$ samples, generate a random uniform and multiply it by the total mass (starting with total mass 1);
choose the index $j$ such that the random mass is less than or equal to sum of the first $j$ sampling probabilities but greater than the $j-1$st; \textcolor{red}{is this explanation right?}
subtract the sampling probability for the $j$th element from the total mass, remove the $j$th element from the sampling probabilities and indices, and repeat.
This also requires $n$ uniform PRNs.
\end{enumerate}
\item The Python \texttt{random} library source code is at \url{https://hg.python.org/cpython/file/2.7/Lib/random.py}.
\begin{enumerate}
\item The random number generator functions rely on generating sequences of random bits.
These can then be turned into integers or numbers on a continuous interval.
\item \texttt{random.choice} takes a sequence as input, generates a random uniform number and multiplies it by the length of the sequence, and returns the element with the nearest integer index.
\item \texttt{random.shuffle} implements the Knuth shuffle. 
The number of PRNs required is equal to the length of the sequence to shuffle.
\item \texttt{random.sample} does equal probability sampling \textbf{without replacement}, like an SRS.
There are two implementations.
The first uses a discard pool to track which elements have been selected.
This is the chosen method if the sample size to population size ratio is large.
This requires $k$ PRNs and the algorithm matches R's implementation.
The second algorithm does not discard elements that have already been sampled, but does sampling \textit{with replacement}.
At each step, the function checks if the selected item is already in the sample.
If so, then it tries until it samples something not yet in the sample.
This method requires \textit{at least} $k$ PRNs.
\end{enumerate}
\item The \texttt{numpy.random} library source code is available on GitHub at \url{https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/mtrand.pyx}.
\begin{enumerate}
\item \texttt{np.random.choice} does sampling with/without replacement and with equal or unequal probabilities.
This is the closest thing to R's \texttt{sample}.
All of the four varieties of sampling are implemented the same way.
SRS sampling relies on \texttt{permutation}, which shuffles an array.
\end{enumerate}
\end{itemize}



\section{May 30, 2016: Thoughts on coding samples}

Suppose the $n$ elements of the population are in a canonical order.
There is a one-to-one map between strings of at most $n$ bits with exactly $k$ nonzero bits
and samples of size $k$. 
Since $n$ and $k$ are known, we can suppose without loss of generality that $k \le n-k \le n/2$
(otherwise, code the elements omitted from the sample instead of the included elements).

Code a sample by listing bits up to the $k$th nonzero bit, not inclusive.
This completely determines the sample.
If the code contains $m$ bits, it specifies the first $k-1$ sampled elements, and then the $(m+1)$st element of the population is included in the sample.
Elements $m+2, \dots, n-1, n$ are not.
This is a \textbf{prefix code}: no complete code is the beginning of another code.

Using this coding, the total number of bits needed to specify all possible samples:
\begin{itemize}
\item there is  $1 = {{k-1} \choose {0}}$ sample that takes exactly $k-1$ bits to code, namely all $1$s
\item there are $k = {{k} \choose {1}}$ samples that take exactly $k$ bits: the samples only omit one of the first $k$ elements
\item there are ${{k+1} \choose 2} $ that take exactly $k+1$ bits
\item there are $ {{k+2} \choose 3} $ that take exactly $k+2$ bits
\item there are $ {{k+\ell-1} \choose {\ell}} $ that take exactly $k+\ell-1$ bits. Equivalently, there are ${{\ell} \choose {\ell - k + 1}}$ that take exactly $\ell$ bits.
\item there are $ {{n-2} \choose {n - k+1}} = {{n-2} \choose {k-1}} $ that take exactly $n-2$ bits
\item there are $ {{n-2} \choose {k-1}} $ samples that take exactly $n-1$ bits: they are exactly the $ {{n-2} \choose {k-1}} $ codes of length $n-2$ bits with an additional $0$ appended. They encode samples where the last element of the population is included.
\end{itemize}
Adding up all the ways to code samples, the number of bits for the entire code is
%$$ b = (n-1) + \sum_{\ell=k}^{n-1} \ell {{\ell-1} \choose {\ell-k}}. $$  (we should be able to simplify this)
$$ b = (n-1)  {{n-2} \choose {k-1}} + \sum_{\ell=k-1}^{n-2} \ell {{\ell} \choose {\ell-k+1}}.$$

\begin{align*}
 b &= (n-1)  {{n-2} \choose {k-1}} + \sum_{\ell=k-1}^{n-2} \ell {{\ell} \choose {k-1}}
\end{align*}

Theoretical lower bound by Shannon entropy:

$$ H = \log_2 {{n} \choose {k}}.$$


\section{May 17, 2016}
\begin{itemize}
\item \textbf{Permutation testing with one sample:} \\
We have $N$ observations and we're doing some permutation test with them.
To approximate the null distribution, we want to sample uniformly at random from all $N!$ permutations of the observations.
Is it possible to obtain all of these permutations, or are we constrained by the period of the PRNG?
\begin{itemize}
\item I am totally open to changing notation! This is temporary.
\item Suppose the period of the PRNG is $\mathcal{P}$.
\item Suppose the permutation algorithm takes $K$ operations.
\item If $K \equiv 0 \mod \mathcal{P}$, then the PRNG will start over at some point. If $\mathcal{P}/K < N!$, then we can't reach all possible permutations.
Otherwise, we're in good shape and we will just start to repeat permutations before the PRNG reaches the end of its period.
\item What happens if $\mathcal{P}/K < N!$ but $K$ does not divide $\mathcal{P}$?
\end{itemize}
\item What is the ``best'' way to do permutations to avoid reaching the end of the period?
There are two issues at tension: the period of the PRNG and the computational complexity of the PRNG and shuffling algorithm.
We want to balance computational efficiency with correctness.
\item What happens if we generate pseudo-random numbers in a distributed fashion?
Obviously one has to set the seed differently for each thread, but does this improve the risk of repeating?
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{refs}


\end{document}