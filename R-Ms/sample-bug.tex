\documentclass[12pt]{article}
\usepackage[breaklinks=true]{hyperref}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs, multicol, multirow}
\usepackage[nohead]{geometry}
\usepackage[singlespacing]{setspace}
\usepackage[bottom]{footmisc}
\usepackage{floatrow}
\usepackage{float,graphicx}
\usepackage{caption}
\usepackage{indentfirst}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}{\mathbb{I}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution (can probably skip this)
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\renewcommand{\baselinestretch}{1.5}

\title{Simple Random Sampling: Not So Simple}
\author{Kellie Ottoboni and Philip~B. Stark}
\date{\today}
\begin{document}
\maketitle


\begin{abstract}
R (Version 3.5.0) generates random integers between $1$ and $m$
by multiplying random floats by $m$, taking the floor, and adding $1$ to the result.
It is well known that quantization effects inherent in this approach result in a 
non-uniform distribution on $\{ 1, \ldots, m\}$.
The difference, which depends on $m$, can be substantial.
Because the \texttt{sample} function in R relies on generating random integers,
random sampling in R is also biased.
There is an easy fix, namely, construct random integers directly from random bits, rather than
multiplying a random float by $m$.
That is the strategy taken in Python's \texttt{numpy.random.randint()} function, among
others.
\end{abstract}


%\newpage

%\section{Introduction}
A textbook way to generate a random integer on 
$\{1, \dots, m\}$ is to start with $X \sim U[0,1)$ and define $Y \equiv 1 + \lfloor mX \rfloor$. 
If $X$ is truly uniform on $[0,1)$, $Y$ is then uniform on $\{1, \dots, m\}$.
However, if $X$ has a discrete distribution derived by scaling a pseudorandom binary integer, 
the resulting distribution is not uniformly distributed on 
$\{1, \ldots, m \}$ even if the underlying pseudorandom number generator 
(PRNG) is perfect (unless $m$ is a power of 2):

\begin{theorem}[\citet{knuth_art_1997}] % p.133
Suppose $X$ is uniformly distributed on $w$-bit binary numbers, and
let $Y_m \equiv 1 + \lfloor mX \rfloor$.
Let $p_+(m) = \max_{1 \le k \le m} \Pr\{Y_m = k\}$ and $p_-(m) = \min_{1 \le k \le m} \Pr\{Y_m = k\}$.
There exists $m < 2^w$ such that, to first order, 
$p_+(m)/p_-(m) = 1 + m2^{-w+1}$.
\end{theorem}

The algorithm that R (Version 3.5.0) \citep{R_2018} uses to generate uniform random integers
has this issue (albeit in a slightly more complicated form, because, depending on $m$,
R uses pseudorandom binary integers of different lengths). 
Because \texttt{sample} relies on random integers, it inherits the problem.

R uses \texttt{unif\_rand} to generate pseudorandom numbers with word size at most $w=32$.
To generate integers with a larger word size, R combines two $w$-bit integers to obtain an integer with 50 to 53 bits (depending the chosen PRNG). 

A better way to generate random elements of $\{1, \dots, m\}$ is to use pseudorandom bits directly. 
The integer $m$ can be represented with $\mu = \lceil \log_2(m) \rceil$ bits. 
To generate a pseudorandom integer between $1$ and $m$, first generate $\mu$ pseudorandom bits (for instance, by taking the most significant $\mu$ bits from the PRNG output).  
If that binary number $M$ is larger than $m-1$, discard it.
Repeat until the $\mu$ bits represent an integer $M$ that is less than $m$. 
Return $M+1$.\footnote{%
   See \citet{knuth_art_1997} p.114.
}
This procedure might discard almost half the draws if $m$ is slightly larger than a power of $2$,
but if the input bits were IID Bernoulli(1/2), the resulting integers will be uniformly distributed.
This is how the Python function \texttt{numpy.random.randint()} (Version 1.14) generates pseudorandom integers.\footnote{%
However, Python's built-in \texttt{random.choice()} (Versions 2.7 through 3.6) does something else biased: it finds the closest integer to $mX$, where $X$ is a binary fraction between 0 and 1.
}

The R \texttt{sample} function has a branch in its logic depending on the number of elements
in the population to be sampled. 
It uses \texttt{ru} when $m >= 2^{31}$ and \texttt{rand\_unif} when $m < 2^{31}$.\footnote{
A different function, \texttt{sample2}, is called when $m > 10^7$ and $k < m/2$.
It uses the same flawed method of generating pseudorandom integers.
}
The nonuniformity of selection probabilities is largest when $m$ is just below $2^{31}$. 
In that case, \texttt{sample} calls \texttt{unif\_rand}, which gives outputs with word size $w=32$. 
The maximum ratio of selection probabilities approaches $2$ as $m$ increases to the cutoff $2^{31}$, or about 2 billion. 
Even if $m$ is close to 1 million, the ratio is about $1.0004$.

When $m \ge 2^{31}$, R calls \texttt{ru()}  to produce a pseudorandom number with word length at least $w=50$ bits and at most $w=53$ bits (depending the chosen PRNG). 
The algorithm uses two pseudorandom numbers, $r_1$ and $r_2$, to produce one with greater precision,
$$ \texttt{ru}() = \frac{\lfloor U r_1 \rfloor + r_2}{U},$$

\noindent where $U = 2^{25}-1$.
In theory, this method would produce equidistant pseudorandom numbers on $[0, 1)$ by using the entire 53-bit mantissa of floating point numbers.
It fails due to floating point arithmetic.

To illustrate the issue, suppose that the PRNG generating $r_1$ and $r_2$ produces 32-bit integers and that floating point numbers 
are represented in base 2 with 4 bits of precision.
Let $U=2^4$.
Imagine that $r_1 = 0$, represented as $0.000 \times 2^0$.
Then the possible values of $\texttt{ru}$ would be equally spaced values, separated by a distance of $U^{-1}$.
Instead, imagine $r_1$ is the largest 32-bit decimal that the PRNG can generate.
In floating point, $r_1$ is represented as $1.000 \times 2^{-1}$.
Multiplying by $U$ only changes the exponent from $-1$ to $3$.
In floating point arithmetic, adding $\lfloor U r_1 \rfloor$ to small values of $r_2$ will result in underflow:
for $r_2 < 1.000 \times 2^{0}$, $\lfloor U r_1 \rfloor + r_2 = \lfloor U r_1 \rfloor$. 
Moreover, $\lfloor U r_1 \rfloor + 1.000 \times 2^{0} = \lfloor U r_1 \rfloor + 1.001 \times 2^{0} = \lfloor U r_1 \rfloor + 1.111 \times 2^{0}$ and so forth.
In this case, $\texttt{ru}$ will give equally spaced values separated by a distance of $2\times U^{-1}$.
% https://github.com/wch/r-source/blob/5a156a0865362bb8381dcd69ac335f5174a4f60c/src/main/RNG.c

Knuth's theorem no longer applies to the ratio of selection probabilities in the branch of the logic that calls $\texttt{ru}$, as values aren't truly uniformly distributed.
In fact, the nonuniformity of pseudorandom numbers might exacerbate the pseudorandom integer generation problem further.
This could be avoided by using pseudorandom bits directly to gain precision, for example by
concatenating two 32-bit pseudorandom numbers or by using a 64-bit PRNG (\cite{marsaglia_64bit_2004}).

\begin{table}[h]
\caption{Maximum ratio of selection probabilities for different population sizes $m$. 
R constructs random integers using random binary integers with different numbers of bits (word lengths) depending on $m$:
if $m < 2^{31}$, the word length is between $25$ and $32$, otherwise it may be up to $53$ bits.
The maximum ratio of selection probabilities is based on Knuth's theorem; 
for $m > 2^{31}$, the maximum may be even larger due to nonuniformity introduced by the \texttt{ru} function.
}
\begin{center}
\begin{tabular}{|c|c|r|}

\hline
Population size ($m$) & Word length ($w$) & Maximum ratio of selection probabilities\\
\hline 
$10^6$ & 32 & 1.0004 \\
$10^9$ & 32 & 1.466 \\
 $2^{31}-\epsilon$ & 32 & 2 \\
$2^{31}+\epsilon$ & 53 & $1 + 2.3 \times 10^{-7}$ \\
$10^{12}$ & 53 & $1.0001$ \\
$10^{15}$ & 53 & $1.11$ \\
\hline

\end{tabular}
\end{center}
\label{tab}
\end{table}%

We recommend that the R developers replace the current algorithm for generating pseudorandom integers with the masking algorithm
and use pseudorandom bits directly to generate pseudorandom numbers with more than 32 bits of precision.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}