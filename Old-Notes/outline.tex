\documentclass[12pt]{article}
\usepackage[breaklinks=true]{hyperref}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
\usepackage[margin=0.75in]{geometry}
\usepackage[singlespacing]{setspace}
\usepackage[bottom]{footmisc}
\usepackage{floatrow}
\usepackage{float,graphicx}
\usepackage{enumerate}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}{\mathbb{I}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution 
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}

\title{Notes: PRNGs and Permutations}
\author{Kellie Ottoboni}
\date{Draft \today}
\begin{document}
\maketitle

%\newpage

%\begin{abstract}


%\end{abstract}

%\newpage
\section{June 20, 2016: Random Integer Generation}
\texttt{numpy.random.randint} (from numpy version 1.11.0) is a function that generates uniformly distributed random integers.
The user specifies lower and upper bounds, as well as the number of integers to generate and the data type of the result.
Depending on the data type specified, the function uses a different way to generate the random integers.
Options for the data type are \texttt{bool}, \texttt{int8}, \texttt{int16}, \texttt{int32}, \texttt{int64}, \texttt{uint8}, \texttt{uint16}, \texttt{uint32}, and \texttt{uint64}.
The default data type argument is \texttt{numpy.int}, which is of type \texttt{int64}.
The way that these random numbers are generated is in the C code at \url{https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/randomkit.c} inside the function \texttt{rk\_random\_uint64}.
Some binary operations are done to generate the random integers:

\begin{enumerate}
\item{ Take the smallest bit \texttt{mask} greater than or equal to the smallest power of two greater than the desired range by applying bitwise or operators. (For example, if the desired range is $35$, then this sequence of operations will yield

\begin{align*}
mask &= 0100011 = 35\\
mask &\leftarrow (0100011 \vert 0010001) = 0110011 = 51 \\
mask &\leftarrow (0110011 \vert 0011001) = 0111011 = 59 \\
mask &\leftarrow (0111011 \vert 0011101) = 0111111 = 63 \\
& \vdots
\end{align*}

so \texttt{mask} $ = 0111111 = 63$.)
}
\item{ Generate a random integer (32-bit integer if the range is less than the maximum unsigned long value that can be stored ($2^{32}-1$), a 64-bit integer otherwise); 
apply bitwise and  to choose the minimum of the random bit sequence and \texttt{mask}
(for example, if we generate $19 = 0010011$ and \texttt{mask} $= 63 = 0111111$, then $19 \& \texttt{mask}  = 0010011 = 19$);
repeat until the value generated is less than the range.
}
\item Add this value to the lower bound.
\end{enumerate}

\todo{It seems like this algorithm would give a higher probability of generating the upper bound.
Are all integers actually equally likely?}\\

R doesn't have a built-in uniform random integer generator. 
Typically, to obtain random integers between $1$ and $m$, inclusive, one would generate a random uniform number $X$ on $[0, 1)$, then take
$$Y = 1 + \lfloor m X \rfloor.$$
In theory, that's fine. 
But in practice, $X$ is not really $U[0,1)$ but instead is derived by normalizing something that's uniform on $w$-bit integers. 
Then, unless $m$ is a power of $2$, the distribution of $Y$ isn't uniform on $\{1,\dots,m\}$. 
For $m<2^w$, the ratio of the largest to smallest selection probability is, to first order, $1+m^{2-w}$.
For $m=109$ and $w=32$, $1+m^{2-w}\approx1.233$. 
That could easily matter.




\section{June 19, 2016: Notes on Random\_Sample Algorithm}
The setting is using Random\_Sample algorithm to generate a simple random sample of $k$ items from $n$.
First, let's show that $\pr(i \in S)$ is equal for all $i = 1, \dots, n$.
For $k=1$, we're in the base case of sampling from $1, \dots, n$ with equal probability, so $\pr(i \in S) = \frac{1}{n}$.
Suppose that $\pr(i \in S) = \frac{s}{n}$ for $s = 1, \dots, t-1$.
We'll show that this is true for $s = t$ by induction.
Let $S^s$ denote the SRS generated by the algorithm at recursion depth $s$, for $s \leq k$.
That is, $S^s$ is a SRS of size $s$ from $\{1, \dots, n - s\}$.
At the top level of recursion, we have
\begin{align*}
\pr(i \in S^t) &= \pr(i \notin S^{t-1}, \text{choose } i) + \pr(i \in S^{t-1}, \text{don't choose } i) \\
&=  \pr(i \notin S^{s-1})\pr(\text{choose } i) + \pr(i \in S^{s-1})\pr(\text{don't choose } i) \tag*{by independent sampling}\\
&= \left(1-\frac{t-1}{n}\right)\frac{1}{n} + \frac{t-1}{n}\frac{n-1}{n} \\
&= \frac{1}{n} + \frac{t-1}{n} \\
&= \frac{t}{n}
\end{align*}

\noindent Suppose we consider subsets $\Omega$ instead of single elements.
We hope that the following holds: if $| \Omega | = j$, then $\pr( \Omega \subset S) = \frac{{ k \choose j }}{{n \choose k}}$.
Now we have induction in two dimensions: the cardinality of $\Omega$ and the recursion depth.
\todo{prove this.}
%Above establishes the induction for recursion depth:
%we have the relation $p_{1, t} = p_{1, t-1} + \frac{1}{n}$.
%
%On the other hand, let's vary the subset size.
%The base case is the same.
%Then, supposing that the relation holds for subsets of size at most $j-1$, then for some $\Omega$ with $| \Omega | =  j-1$, we have
%
%\begin{align*}
%\pr( \{ \Omega \cup i_j \} \subset S^{t}) &= \pr( \Omega \subset S^{t}, i_j \notin S^t, \text{choose }i_j) + \sum_{\ell=1}^{j-1} \pr( \Omega\textcolor{red}{without} i_{\ell} \subset S^{t}, i_j \in S^t, \text{choose }i_\ell) \\
%&=  \pr( \Omega \subset S^{t}, i_j \notin S^t)\pr( \text{choose }i_j) + \sum_{\ell=1}^{j-1} \pr( \Omega\textcolor{red}{without} i_{\ell} \subset S^{t}, i_j \in S^t)\pr(\text{don't choose }i_\ell) \\
%&= p_{| \Omega |, t} (1- p_{1, t - |\Omega |}) + \sum_{\ell=1}^{j-1} p_{| \Omega | - 1, t} p_{1, t} (1 - p_{1, t-|\Omega |}) \\
%&= p_{j-1, t} (1- p_{1, t -j+1}) + \sum_{\ell=1}^{j-1} p_{j - 2, t} p_{1, t} (1 - p_{1, t-j+1})\\
%&= p_{j-1, t} \left(1- \frac{t-j+1}{n}\right) + \sum_{\ell=1}^{j-1} p_{j - 2, t} \frac{t}{n} \left(1 - \frac{t-j+1}{n}\right) \tag*{by the first inductive relation}\\
%&= p_{j-1, t} \left(1- \frac{t-j+1}{n}\right) +  p_{j - 2, t}(j-1)  \frac{t}{n} \left(1 - \frac{t-j+1}{n}\right) \\
%\end{align*}
%
%Similarly for any subset $\Omega = \{i_1, \dots, i_s\} \subset \{1, \dots, n\}$ such that $| \Omega | \leq | S |$, we have
%
%\begin{align*}
%\pr(\Omega \subset S^t) &= \pr(\Omega \subset S^{t-1}, \text{don't choose elements from } \Omega) + \sum_{j} \pr(\{\Omega \textcolor{red}{without} i_j \} \subset S^{t-1}, \text{choose } i_j) \\
%&=  \pr(\Omega \subset S^{t-1}) \pr(\text{don't choose elements from } \Omega) + \sum_{j} \pr(\{\Omega \textcolor{red}{without} i_j \} \subset S^{t-1})\pr( \text{choose } i_j) \\
%&=  p_{|\Omega|, t-1} \frac{n - | \Omega |}{n} + \sum_{j} p_{|\Omega | -1, t-1} \frac{1}{n} \\
%&= p_{|\Omega|, t-1} \frac{n - | \Omega |}{n}  + |\Omega | p_{|\Omega | -1, t-1} \frac{1}{n}
%\end{align*} 

\section{June 6, 2016: Notes}
There are several moving parts:
\begin{itemize}
\item \textbf{How are samples encoded?} 
We describe samples with a code, which is a sequence from a finite set of elementary symbols.
If we use bits to code samples (1 if the item is selected, 0 if not), the Shannon lower bound on the number
of words in the code is $H = \log_2 {n \choose k}$.
If we use a different set of elementary symbols for our code, this lower bound will be different.
In particular, if there are $s$ symbols to choose from, then $H = \log_s {n \choose k}$.
\item \textbf{What does the PRNG output?}
Do we imagine that the PRNG gives a sequence of 32 bits?
Does it give a number in the interval $[0, 1]$?
Does it give us an integer on some interval?
We need to be specific about this, as it will affect our counting of states.
It might be the case that to generate a number in $[0, 1]$, it requires several random bits to be generated.
\textbf{Answer:} let's think of it as outputting a 32-bit integer.
The Mersenne Twister is k-distributed to 32 bits for $1 \leq k \leq 623$.
\item What happens when we reach the end of the PRNG's period, 
but the period is not a multiple of the number of PRNs needed to generate a sample?
\end{itemize}

Define $H_{nk} = \log_2 {n \choose k}$.
Total number of bits from PRNG = $period \times integer-length$. 
This divided by $H_{nk}$ is greater than or equal to the total number of samples of k of n without recycling/per period.


Do this calculation for LCG: put in $a, b, m$, gives upper bound on period.
Then take $n, k$ pair and do calculation.
Do randU and Mersenne Twister as well.


\subsection{Period length of linear congruential generator}
(from Wikipedia)

The period of a general mixed congruential generator is at most m, and for some choices of factor a much less than that. The mixed congruential generator will have a full period for all seed values if and only if:

\begin{enumerate}
\item $m$ and the offset $c$ are relatively prime,
\item $a-1$ is divisible by all prime factors of $m$,
\item $a-1$ is divisible by $4$ if $m$ is divisible by $4$.
\end{enumerate}

These three requirements are referred to as the Hull-Dobell Theorem.
While LCGs are capable of producing pseudorandom numbers which can pass formal tests for randomness, this is extremely sensitive to the choice of the parameters $c$, $m$, and $a$.

\subsection{Factorial bounds}

Bound on binomial coefficients

$$ \frac{2^{nH(q)}}{n+1} \leq {n \choose k} \leq 2^{nH(q)}$$
where $H(q) = -q \log_2(q) - (1-q)\log_2(1-q)$, and $q = k/n$.


\section{Standard Implementations}
\subsection{Psuedo-random number generators}
At the time of writing this, Python (version 2.6 and up), R (version 3.3.0), SAS (version 9.4), SPSS (version 23.0), and Matlab (R2016a, version 9.0) use the Mersenne Twister as their default PRNG.
Until April 2015, Stata used the KISS PRNG; the recent upgrade to version 14.0 introduced the Mersenne Twister.
Excel 2003 used the Wichmann-Hill generator, but implemented it incorrectly; Excel 2007 attempted to fix the bug but failed (McCullough 2008).
Now, Excel 2010 and more recent versions use the Mersenne Twister.

\subsection{Sampling algorithms}
\begin{itemize}
\item The naive implementation is to generate $n$ random numbers between $1$ and $n$ (shuffle in place or permute),
sort the observations according to the random index they're assigned,
and take the top $k$ to be the sample.
This is inefficient and requires up to $n$ PRNs to get the random ordering.
\item Someone has put the R base C source code in an unaffiliated GitHub repository at \url{https://github.com/SurajGupta/r-source/blob/master/src/main/random.c}.
Here, we see that R's \texttt{sample} (version 3.3.0) is broken down into four separate functions.
The functions sample by first randomly selecting indices for the new sample, then copying the original data into the return vector using the new indices.
\begin{enumerate}
\item \textbf{Equal probability sampling without replacement:} 
Start with a population of size $n$.
For each of $k$ samples, generate a uniform PRN;
multiply it by the current population size $\tilde{n}$ and take the floor, then add one -- this is the selected index;
decrement the current population size $\tilde{n}$ by one and put the last remaining index in the place of the selected index.
This only requires $k$ uniform PRNs.
\item \textbf{Equal probability sampling with replacement:} 
for each of $k$ samples, generate a uniform PRN;
multiply it by the population size $n$ and add one, then take the floor function.
This also requires $k$ uniform PRNs.
\item \textbf{Unequal probability sampling with replacement:} 
Sort sampling probabilities and the corresponding indices in descending order; 
compute cumulative probabilities; 
for each of $n$ samples, generate a random uniform and select the index $j$ such that the random number is less than or equal to the $j$th cumulative probability but not the $j+1$st.  
This requires $n$ uniform PRNs.  
OR Walker's alias method for large samples.
\item \textbf{Unequal probability sampling without replacement:} 
Sort sampling probabilities and the corresponding indices in descending order; 
for each of $n$ samples, generate a random uniform and multiply it by the total mass (starting with total mass 1);
choose the index $j$ such that the random mass is less than or equal to sum of the first $j$ sampling probabilities but greater than the $j-1$st; \textcolor{red}{is this explanation right?}
subtract the sampling probability for the $j$th element from the total mass, remove the $j$th element from the sampling probabilities and indices, and repeat.
This also requires $n$ uniform PRNs.
\end{enumerate}
\item The Python (version 2.7) \texttt{random} library source code is at \url{https://hg.python.org/cpython/file/2.7/Lib/random.py}.
\begin{enumerate}
\item The random number generator functions rely on generating sequences of random bits.
These can then be turned into integers or numbers on a continuous interval.
\item \texttt{random.choice} takes a sequence as input, generates a random uniform number and multiplies it by the length of the sequence, and returns the element with the nearest integer index.
\item \texttt{random.shuffle} implements the Knuth shuffle. 
The number of PRNs required is equal to the length of the sequence to shuffle.
\item \texttt{random.sample} does equal probability sampling \textbf{without replacement}, like an SRS.
There are two implementations.
The first uses a discard pool to track which elements have been selected.
This is the chosen method if the sample size to population size ratio is large.
This requires $k$ PRNs and the algorithm matches R's implementation.
The second algorithm does not discard elements that have already been sampled, but does sampling \textit{with replacement}.
At each step, the function checks if the selected item is already in the sample.
If so, then it tries until it samples something not yet in the sample.
This method requires \textit{at least} $k$ PRNs.
\end{enumerate}
\item The \texttt{numpy.random} library (version 1.11.0) source code is available on GitHub at \url{https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/mtrand.pyx}.
\begin{enumerate}
\item \texttt{np.random.choice} does sampling with/without replacement and with equal or unequal probabilities.
This is the closest thing to R's \texttt{sample}.
\begin{enumerate}

\item \textbf{Equal probability sampling without replacement:} 
They do something close to the naive implementation:
permute the indices $1$ through $n$,
take the first $k$ of the permuted indices,
and the units with these indices form the sample.
This requires $n$ random numbers to perform the Fisher-Yates shuffle, which gets called from the permutation function.
\end{enumerate}
\end{enumerate}

\end{itemize}



\section{May 30, 2016: Thoughts on coding samples}

Suppose the $n$ elements of the population are in a canonical order.
There is a one-to-one map between strings of at most $n$ bits with exactly $k$ nonzero bits
and samples of size $k$. 
Since $n$ and $k$ are known, we can suppose without loss of generality that $k \le n-k \le n/2$
(otherwise, code the elements omitted from the sample instead of the included elements).

Code a sample by listing bits up to the $k$th nonzero bit, not inclusive.
This completely determines the sample.
If the code contains $m$ bits, it specifies the first $k-1$ sampled elements, and then the $(m+1)$st element of the population is included in the sample.
Elements $m+2, \dots, n-1, n$ are not.
This is a \textbf{prefix code}: no complete code is the beginning of another code.

Using this coding, the total number of bits needed to specify all possible samples:
\begin{itemize}
\item there is  $1 = {{k-1} \choose {0}}$ sample that takes exactly $k-1$ bits to code, namely all $1$s
\item there are $k = {{k} \choose {1}}$ samples that take exactly $k$ bits: the samples only omit one of the first $k$ elements
\item there are ${{k+1} \choose 2} $ that take exactly $k+1$ bits
\item there are $ {{k+2} \choose 3} $ that take exactly $k+2$ bits
\item there are $ {{k+\ell-1} \choose {\ell}} $ that take exactly $k+\ell-1$ bits. Equivalently, there are ${{\ell} \choose {\ell - k + 1}}$ that take exactly $\ell$ bits.
\item there are $ {{n-2} \choose {n - k+1}} = {{n-2} \choose {k-1}} $ that take exactly $n-2$ bits
\item there are $ {{n-2} \choose {k-1}} $ samples that take exactly $n-1$ bits: they are exactly the $ {{n-2} \choose {k-1}} $ codes of length $n-2$ bits with an additional $0$ appended. They encode samples where the last element of the population is included.
\end{itemize}
Adding up all the ways to code samples, the number of bits for the entire code is
%$$ b = (n-1) + \sum_{\ell=k}^{n-1} \ell {{\ell-1} \choose {\ell-k}}. $$  (we should be able to simplify this)
$$ b = (n-1)  {{n-2} \choose {k-1}} + \sum_{\ell=k-1}^{n-2} \ell {{\ell} \choose {\ell-k+1}}.$$

\begin{align*}
 b &= (n-1)  {{n-2} \choose {k-1}} + \sum_{\ell=k-1}^{n-2} \ell {{\ell} \choose {k-1}}
\end{align*}

Theoretical lower bound by Shannon entropy:

$$ H = \log_2 {{n} \choose {k}}.$$


\section{May 17, 2016}
\begin{itemize}
\item \textbf{Permutation testing with one sample:} \\
We have $N$ observations and we're doing some permutation test with them.
To approximate the null distribution, we want to sample uniformly at random from all $N!$ permutations of the observations.
Is it possible to obtain all of these permutations, or are we constrained by the period of the PRNG?
\begin{itemize}
\item I am totally open to changing notation! This is temporary.
\item Suppose the period of the PRNG is $\mathcal{P}$.
\item Suppose the permutation algorithm takes $K$ operations.
\item If $K \equiv 0 \mod \mathcal{P}$, then the PRNG will start over at some point. If $\mathcal{P}/K < N!$, then we can't reach all possible permutations.
Otherwise, we're in good shape and we will just start to repeat permutations before the PRNG reaches the end of its period.
\item What happens if $\mathcal{P}/K < N!$ but $K$ does not divide $\mathcal{P}$?
\end{itemize}
\item What is the ``best'' way to do permutations to avoid reaching the end of the period?
There are two issues at tension: the period of the PRNG and the computational complexity of the PRNG and shuffling algorithm.
We want to balance computational efficiency with correctness.
\item What happens if we generate pseudo-random numbers in a distributed fashion?
Obviously one has to set the seed differently for each thread, but does this improve the risk of repeating?
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{refs}


\end{document}