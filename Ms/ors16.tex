\documentclass[12pt]{article}
%\usepackage[breaklinks=true]{hyperref}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs, multicol, multirow}
\usepackage[nohead, margin=0.75in]{geometry}
\usepackage[singlespacing]{setspace}
\usepackage[bottom]{footmisc}
\usepackage{floatrow}
\usepackage{float,graphicx}
\usepackage{caption}
\usepackage{indentfirst}
\usepackage{lscape}
\usepackage{floatrow}
\usepackage{epsfig}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks=true,
            urlcolor=RawSienna,
            linkcolor=RawSienna,
            citecolor=NavyBlue]{hyperref}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}{\mathbb{I}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution (can probably skip this)
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\renewcommand{\baselinestretch}{1.5}

\begin{document}

\title{Simple Random Sampling: Not Simple}
\author{Kellie Ottoboni
\and
Ron L. Rivest
\and
Philip B.~Stark 
}

\date{draft \today}




\maketitle

\begin{abstract}
\small
A simple random sample (SRS) of size $k$ from a population of size $n$ is a sample drawn 
at random in such a way that every subset of $k$ of the $n$ items is equally likely to be selected. 
The theory of inference from SRSs is fundamental in statistics;
many statistical techniques and formulae assume that the data are an SRS.
True SRSs are rare; in practice, people tend to draw samples by using pseudo-random number generators 
(PRNGs) and algorithms that map a set of pseudo-random numbers into a subset of the population. 
Most statisticians take for granted that the software they use ``does the right thing,''
producing samples that can be treated as if they are SRSs.
In fact, the PRNG algorithm and the algorithm for drawing samples using the PRNG matter
enormously.
Using basic counting principles, we show that some widely used methods cannot generate all subsets of size $k$.
In simulations, we demonstrate that the subsets that they do generate do not have equal frequencies, which
introduces bias and makes uncertainty calculations meaningless.
We compare the ``randomness'' and computational efficiency of commonly-used PRNGs to a PRNG 
based on the SHA-256 hash function, which avoids these pitfalls because its state space is countably infinite.
We propose several best practices for researchers using PRNGs, including the wide adoption of hash function based PRNGs.
\end{abstract}


\newpage
\tableofcontents
\newpage 

\section{Introduction}
Random sampling is one of the most fundamental tools in Statistics.
It is used to conduct surveys, including opinion surveys, population surveys like the census, and litigation; 
to run medical, agricultural, and marketing experiments; 
quality control in industry and auditing in finance and elections;
and countless other purposes.
Simple random sampling refers to drawing $k \leq n$ items from a population of $n$ items,
in such a way that each of ${n \choose k}$ subsets of size $k$ is equally likely.
Many standard statistical methods assume that the sample is drawn this way, 
or allocated between treatment and control groups this way
(e.g. $k$ of $n$ subjects are assigned to treatment, and the remaining $n-k$ to control).

\begin{itemize}
\item We examine methods for drawing pseudo-random simple random samples. 
We include a discussion of pseudo-random number generators (PRNGs) and 
of algorithms used to select samples using PRNGs.
Among other things, we find bounds on the number of samples that can be generated 
using a variate of PRNGs, for a number of sampling algorithms. 
We also consider how that affects the bias and uncertainty of estimates based on pseudo-random
samples rather than on actual simple random samples.
\item PRNGs considered include linear congruential generators (LCGs, including RANDU)
and the Mersenne Twister. We discuss using cryptographic hash functions to generate PRNs.
\item We conclude with recommendations for best practices using PRNGs to generate random samples.
\end{itemize}



\section{Background}
\subsection{Definition of ``random'' numbers}
Most computers lack the hardware needed to generate truly random numbers. 
Instead, they use algorithms called pseudo-random number generators (PRNGs) to generate
deterministic sequences from an initial ``seed,'' which generally can be set by the user,
for instance to an externally generated random value.
Each time a number is generated, the PRNG's ``state'' changes.

Depending on the quality of the PRNG, the sequences behave more or less like sequences of random numbers.
How does one gauge ``how random'' sequences from a PRNG are?
A PRNG yields sequences of random numbers on the interval $[0, 1]$ or over the binary set $\{0, 1\}$.
The sequences output by such PRNGs should be statistically indistinguishable from IID $U(0,1)$ sequences or
IID Bernoulli$(1/2)$ sequences, respectively.
The tests for the bit sequence case applies to the $U(0,1)$ case as well, because there is a one-to-one relationship
between sequences of bits and integers.
Namely, every sequence of $k$ bits corresponds to an integer on $0, 1, \dots, 2^k-1$, and scaling by $2^k$
yields a unique value on $[0, 1)$. (cite L'Ecuyer, Simard)

There are a multitude of ways to test the hypothesis that sequences from a PRNG are indistinguishable 
from random sequences.
\todo{list types of tests + citations: L'Ecuyer Simard TestU01 (2007),  Knuth (1969), Marsaglia DIEHARD tests (1968), NIST tests

\begin{itemize}
\item Two notions of uniformity: 1) uniformly distributed within a sequence with a fixed starting state, 2) the set of sequences of length $t$ that can be hit by the PRNG should be uniformly distributed among the set of all sequences of length $t$ -- this gets at the SRS problem
\item Make sure to define terms like $k$-distributed
\end{itemize}
}

\subsection{Sampling algorithms}
A simple random sample of size $k$ from a population of size $n$ is a sample drawn in such a way that each of the ${n \choose k}$ possible subsets of size $k$ is equally likely.
Given a good source of randomness, there are many ways to operationalize this definition to draw simple random samples.
\subsubsection{Algorithm PIKK (permute indices and keep $k$)}
One basic approach is like shuffling a deck of $n$ cards, then dealing the top $k$: permute the population at random, then take the first $k$ elements of the permutation to be the sample.
There are a number of standard ways to generate a random permutation -- i.e., to shuffle the deck.
If we had a way to generate independent, identically distributed (iid) $U[0,1]$ random numbers, we could sample $k$ out of $n$ as follows:
\todo{create an algorithm environment}
Algorithm PIKK
\begin{itemize}
\item assign iid $U[0,1]$ numbers to the $n$ elements of the population
\item the sample consists of the $k$ items assigned the smallest random numbers (break ties randomly)
\item amounts to generating a random permutation of the population, then taking first $k$.
\item if the numbers really are iid, every permutation is equally likely, and it follows that the first $k$ are an SRS
requires $n$ random numbers (and sorting)
\end{itemize}

This algorithm is inefficient: it requires the generation of $n$ random numbers and then a sorting operation.

\subsubsection{Shuffle}
There are more efficient ways to generate a random permutation than assigning a number to each element and sorting.
One example is the "Fisher-Yates shuffle" or "Knuth shuffle" (Knuth attributes it to Durstenfeld).

\begin{verbatim}
Algorithm Fisher-Yates-Knuth-Durstenfeld shuffle (backwards version)
for i=1, ..., n-1:
    J <- random integer uniformly distributed on {i, ..., n}
    (a[J], a[i]) <- (a[i], a[J])
\end{verbatim}

This algorithm requires the ability to generate independent random integers on various ranges, but doesn't require sorting.
There is also a version suitable for streaming, i.e. generating a random permutation of a list that has an (initially) unknown number of elements.

\begin{verbatim}
Algorithm Fisher-Yates-Knuth-Durstenfeld shuffle (streaming version)
i <- 0
a = []
while there are records left:
    i <- i+1
    J <- random integer uniformly distributed on {1, ..., i}
    if J < i:
        a[i] <- a[J]
        a[J] <- next record
    else:
        a[i] <- next record

\end{verbatim}

\begin{proof}[Proof that the streaming shuffle works]
We prove by induction. The base case $i=1$ is trivial.
At stage $i$, $i>1$, suppose that all $(i-1)!$ permutations of $\{1, 2, \dots, i-1\}$ are equally likely.
$J$ may take on $i-1$ distinct values less than $i$, and the swapping procedure thus yields $(i-1)! \times (i-1)$
possible distinct permutations.
If $J = i$, then there is no swap and $(i-1)$ possible distinct permutations.
In total, there are $(i-1)! \times (i-1 + 1) = i!$ possible distinct permutations, and all are equally likely because
all values of $J$ and permutations of the first $(i-1)$ elements were equally likely.
\end{proof}


\subsubsection{Algorithm \texttt{Random\_Sample} from Cormen et al \todo{cite}}
This is a recursive algorithm that requires only $k$ random integers and does not require sorting.
\todo{turn into pseudocode; prove by recursion that the method works}
\begin{verbatim}
def Random_Sample(n, k, gen=np.random):  # from Cormen et al.
    if k==0:
        return set()
    else:
        S = Random_Sample(n-1, k-1)
        i = gen.randint(1,n) 
        if i in S:
            S = S.union([n])
        else:
            S = S.union([i])
    return S
\end{verbatim}

\subsection{Instances of problems}
The sampling algorithms in the previous section rely on the ability to generate random integers uniformly distributed 
on various ranges.
In this section, we illustrate several known failures in common software packages.
 

\subsubsection{Linear Congruential Generators and RANDU}

\begin{itemize}
\item Linear congruential generators (LCGs) have the form $X_{n+1} = (a X_n + c) \mod m$, for a modulus $m$, 
multiplier $a$, and an additive parameter $c$.
The behavior of LCGs is well-understood from fundamental number theory.
There are three criteria used to choose good values of the parameters of a LCG:
the LCG should generate the full period of length $m$,
the full period sequence $X_1, \dots, X_{m-1}$ should be statistically indistinguishable from random,
and the multiplication and modulus operations should be able to be implemented efficiently in bitwise arithmetic (\todo{cite Hornfeck and Harbrecht}).
For instance, there is theory on which LCGs yield a full period. \todo{cite theorem}

\begin{theorem}[Hull-Dobell Theorem]
The period of an LCG is $m$ for all seeds $X_0$ if and only if
\begin{itemize}
\item $m$ and $c$ are relatively prime,
\item $a-1$ is divisible by all prime factors of $m$, and
\item $a-1$ is divisible by 4 if $m$ is divisible by 4.
\end{itemize}
\end{theorem}

\item For the sake of computational speed, programmers developed LCGs using moduli of the form $m = 2^b$, where
$b$ was the integer word size of the computer.
Such an LCG violates the first principle of choosing good parameters, as no non-prime $m$ can yield an LCG with a full period \todo{cite or prove}.
One particularly nefarious version of this LCG is RANDU, a PRNG promulgated in the 1960s and widely copied \todo{cite}. 
RANDU takes the form

$$X_{n+1} = 65539 X_n \mod 2^{31}.$$

It has a relatively short period ($2^{29}$), all its outputs are odd integers, and it fails the spectral test, which studies the lattice structures of LCGs.
Triples of values from RANDU fall on 15 planes in 3-dimensional space.
\item \todo{mention RANDU blowing up science, finding erroneous crystallographic structure}
\item \todo{mention Marsaglia random numbers fall on the planes}
\end{itemize}


\subsubsection{Wichmann-Hill (1982) \todo{cite} and Excel}
The Wichmann-Hill PRNG is a sum of three LCGs, used to produce random values on $[0, 1)$.
Given three seed values $s_1, s_2, s_3$, the next value $r$ is given by
\begin{align*}
s_1 &= 171 s_1\mod(30269) \\
s_2 &= 172 s_2 \mod(30307) \\
s_3 &= 170 s_3 \mod(30323) \\
r &= (s_1/30269 + s_2/30307 + s_3/30323) \mod(1)
\end{align*}

The Wichmann-Hill generator is generally not considered adequate for statistics, but was (nominally) the PRNG in Excel for several generations. 
Excel did not allow the seed to be set, so analyses were not reproducible.
Moreover, the generator in Excel had an implementation bug that persisted for several generations.
Excel didn't allow the seed to be set so issues could not be replicated, but users reportedly generated negative numbers on occasion.
\todo{cite McCullough, B.D., 2008. Microsoft Excel?s ?Not The Wichmann?Hill? random number generators}

\subsubsection{Stata software}
\todo{ summarize results from Ozier: "Perils of Simulation..."}

\subsubsection{Random integer generation in R}
The sampling and permutation algorithms described above require $k$ or $n$ independent random integers
uniformly distributed on $\{1, \dots, m\}$ for varying values of $m$, depending on the algorithm.
A standard way to generate a random integer is to start with $X \sim U[0,1)$ and define $Y \equiv 1 + \lfloor mX \rfloor$. 
In theory, that's fine. 
But in practice, $X$ is not really $U[0,1)$ but instead is derived by normalizing a PRN that's uniform on $w$-bit integers. 
Then, unless $m$ is a power of 2, the distribution of $Y$ isn't uniform on $\{1, \ldots, m\}$. 

\begin{lemma}
For $m < 2^w$, the ratio of the largest to smallest selection probability is, to first order,  $1+ m 2^{-w}$. (See, e.g., Knuth v2 3.4.1.A.)
\end{lemma}

\begin{proof}
Define $\tilde{X}$ to be a uniform random integer on $\{0, 1, \dots, 2^w - 1\}$.
The selection probability for a particular integer value is 

\begin{align*}
\pr\left(Y = y\right) &= \pr\left(1 + \lfloor mX \rfloor = y\right) \\
&= \pr\left(y-1 \leq mX < y\right) \\
&= \pr\left(\tilde{X} < \frac{y2^w}{m}\right) - \pr\left(\tilde{X} \leq \frac{(-1)y2^w}{m}\right)\\
&= \pr\left(\tilde{X} < \left\lfloor\frac{y2^w}{m}\right\rfloor\right) - \pr\left(\tilde{X} \leq \left\lfloor\frac{(y-1)2^w}{m}\right\rfloor\right)\\
&= 2^{-w}\left(k^+(y-1)- k^-(y-1)\right)
\end{align*}

\noindent where, for fixed $m$, we define $k^-(i) \equiv \min \{k: k2^{-w} \geq i/m\}$ for all $i$,
$k^+(i) \equiv \max \{k : k2^{-w} < i/m \} = k^-(i+1)-1$ for $i = 0, \dots, m-1$
and $k^+(m) \equiv 2^w$.
The maximum ratio of selection probabilities is 

\begin{align*}
\max_{i, j \in \{0, \ldots, m-1\}} \frac{k^+(i) - k^-(i)}{k^+(j) - k^-(j)}
&= \frac{ \max_{i=0}^{m-1} (k^+(i) - k^-(i))}{\min_{i=0}^{m-1} (k^+(i) - k^-(i))} \\
&= \frac{ \max_{i=0}^{m-1} (k^+(i) - k^+(i+1) + 1)}{\min_{i=0}^{m-1} (k^-(i+1) - k^-(i) - 1)} \\
&= \frac{\lceil 2^w/m \rceil + 1}{\lfloor 2^w/m \rfloor -1}.
\end{align*}
\end{proof}
\todo{is this proof right? seems wrong}

For $m = 10^9$ and $w=32$, $1 + m 2^{-w} \approx 1.233$. That could easily matter.

\subsection{The right way to generate PRNs}
\subsubsection{xorShift}
\subsubsection{Mersenne Twister}
\subsubsection{Methods based on cryptographic hash functions}

\section{Results}
\subsection{Pigeonhole arguments}
\subsection{Simulations showing bias}
\section{Hash-function based RNGs}
\section{Discussion}
\subsection{Best Practices}
\section{Conclusions}




\end{document}
