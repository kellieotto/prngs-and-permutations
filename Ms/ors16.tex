\documentclass[12pt]{article}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs, multicol, multirow}
\usepackage[nohead, margin=0.75in]{geometry}
\usepackage[singlespacing]{setspace}
\usepackage[bottom]{footmisc}
\usepackage{floatrow}
\usepackage{float,graphicx}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}


\usepackage{algpseudocode,algorithm,algorithmicx}
\newcommand*\Let[2]{\State #1 $\gets$ #2}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}{Assumption}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}


\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}{\mathbb{I}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution (can probably skip this)
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\renewcommand{\baselinestretch}{1.5}

\begin{document}

\title{Simple Random Sampling: Not So Simple}
\author{Kellie Ottoboni
\and
Ron L. Rivest
\and
Philip B.~Stark 
}

\date{draft \today}




\maketitle

\begin{abstract}
\small
A simple random sample (SRS) of size $k$ from a population of size $n$ is a sample drawn 
at random in such a way that every subset of $k$ of the $n$ items is equally likely to be selected. 
The theory of inference from SRSs is fundamental in statistics;
many statistical techniques and formulae assume that the data are an SRS.
True SRSs are rare; in practice, people tend to draw samples by using pseudo-random number generators 
(PRNGs) and algorithms that map a set of pseudo-random numbers into a subset of the population. 
Most statisticians take for granted that the software they use ``does the right thing,''
producing samples that can be treated as if they are SRSs.
In fact, the PRNG algorithm and the algorithm for drawing samples using the PRNG matter
enormously.
Using basic counting principles, we show that some widely used methods cannot generate all SRSs of size $k$.
In simulations, we demonstrate that the subsets that they do generate do not have equal frequencies, which
introduces bias and makes uncertainty calculations meaningless.
We compare the ``randomness'' and computational efficiency of commonly-used PRNGs to a PRNG 
based on the SHA-256 hash function, which avoids these pitfalls because its state space is countably infinite.
We propose several best practices for researchers using PRNGs, including the wide adoption of hash function based PRNGs.
\end{abstract}


\newpage
\tableofcontents
\newpage 

\section{Introduction}
Random sampling is one of the most fundamental tools in Statistics.
It is used to conduct surveys, including opinion surveys, population surveys like the census, and litigation; 
to run medical, agricultural, and marketing experiments; 
quality control in industry and auditing in finance and elections;
and countless other purposes.
Simple random sampling refers to drawing $k \leq n$ items from a population of $n$ items,
in such a way that each of ${n \choose k}$ subsets of size $k$ is equally likely.
Many standard statistical methods assume that the sample is drawn this way, 
or allocated between treatment and control groups this way
(e.g. $k$ of $n$ subjects are assigned to treatment, and the remaining $n-k$ to control).

\begin{itemize}
\item We examine methods for drawing pseudo-random simple random samples. 
We include a discussion of pseudo-random number generators (PRNGs) and 
of algorithms used to select samples using PRNGs.
Among other things, we find bounds on the number of samples that can be generated 
using a variate of PRNGs, for a number of sampling algorithms. 
We also consider how that affects the bias and uncertainty of estimates based on pseudo-random
samples rather than on actual simple random samples.
\item PRNGs considered include linear congruential generators (LCGs, including RANDU)
and the Mersenne Twister. We discuss using cryptographic hash functions to generate PRNs.
\item We conclude with recommendations for best practices using PRNGs to generate random samples.
\end{itemize}



\section{Background}
\subsection{Randomness and pseudorandomness}
Most computers lack the hardware needed to generate truly random numbers. 
Instead, they use algorithms called pseudo-random number generators (PRNGs) to generate
deterministic sequences from an initial ``seed,'' which generally can be set by the user,
for instance to an externally generated random value.
Each time a number is generated, the PRNG's ``state'' changes.

Depending on the quality of the PRNG, the sequences behave more or less like sequences of random numbers.
How does one gauge ``how random'' sequences from a PRNG are?
A PRNG yields sequences of random numbers on the interval $[0, 1]$ or over the binary set $\{0, 1\}$.
The sequences output by such PRNGs should be statistically indistinguishable from IID $U(0,1)$ sequences or
IID Bernoulli$(1/2)$ sequences, respectively.
Tests for randomness, such as the Diehard battery (\cite{marsaglia_diehard_1995}), the NIST Statistical Test Suite (\cite{soto_statistical_1999,rukhin_statistical_2010}), and the TestU01 suite (\cite{lecuyer_testu01_2007}),
weigh the evidence that sequences produced by PRNGs behave like uniform random numbers in a variety of ways.
These tests include checks for uniformity at different granularities,
unpredictability of sequences of different lengths,
compressibility, and so on.\footnote{
Any tests that check how random bit sequences are also apply to testing randomness of $U[0,1)$ sequences as well.
There is a one-to-one relationship between sequences of bits and integers:
every sequence of $k$ bits corresponds to an integer on $0, 1, \dots, 2^k-1$ (\cite{lecuyer_testu01_2007}).
}

\begin{itemize}
\item Two notions of uniformity: 1) uniformly distributed within a sequence with a fixed starting state, 2) the set of sequences of length $t$ that can be hit by the PRNG should be uniformly distributed among the set of all sequences of length $t$ -- this gets at the SRS problem
\item Make sure to define terms like $k$-distributed
\end{itemize}


\subsection{Sampling algorithms}
A simple random sample of size $k$ from a population of size $n$ is a sample drawn in such a way that each of the ${n \choose k}$ possible subsets of size $k$ is equally likely.
Given a good source of randomness, there are many ways to operationalize this definition to draw simple random samples.

\subsubsection{PIKK (permute indices and keep $k$)}
One basic approach is like shuffling a deck of $n$ cards, then dealing the top $k$: permute the population at random, then take the first $k$ elements of the permutation to be the sample.
There are a number of standard ways to generate a random permutation -- i.e., to shuffle the deck.
If we had a way to generate independent, identically distributed (iid) $U[0,1]$ random numbers\footnote{
Alternatively, one could generate independent and identically distributed random integers $1, \dots, n$.
What matters is not the range of values, but that numbers are uniformly distributed.}, we could sample $k$ out of $n$ as follows:

\begin{algorithm}                      % enter the algorithm environment
\caption{PIKK: Permute indices and keep $k$}          % give the algorithm a caption
\label{PIKK}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]               % enter the algorithmic environment
    \Require $n \geq k \geq 0$
    \Statex
     \State{Assign IID uniform values on $[0,1]$ to the $n$ elements of the population}
     \State{Sort the population according to these values (break ties randomly)}
     \State{Take the top $k$ to be the sample}
\end{algorithmic}
\end{algorithm}

This amounts to generating a random permutation of the population and throwing out all but the first $k$.
If the numbers really are independent and identically distributed, every permutation is equally likely, and it follows that the first $k$ are an SRS.
If permutations are not equiprobable, then samples generated using this algorithm may not be either.

Furthermore, this algorithm is inefficient: it requires the generation of $n$ random numbers and then an $O(n\log n)$ sorting operation.

\subsubsection{Shuffle}
There are more efficient ways to generate a random permutation than assigning a number to each element and sorting.
One example is the``Fisher-Yates shuffle'' or ``Knuth shuffle'' (Knuth attributes it to Durstenfeld).

\begin{algorithm}                      % enter the algorithm environment
\caption{Fisher-Yates-Knuth-Durstenfeld shuffle (backwards version)}          % give the algorithm a caption
\label{FYKD}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]               % enter the algorithmic environment
\For{$i = 1, \dots, n-1$}
    \Let{$J$}{random integer uniformly distributed on $i, \dots, n$}
    \Let{$(a[J], a[i])$}{$(a[i], a[J])$}
\EndFor
\end{algorithmic}
\end{algorithm}

This algorithm requires the ability to generate independent random integers on various ranges, but doesn't require sorting.
There is also a version suitable for streaming, i.e. generating a random permutation of a list that has an (initially) unknown number of elements.

\begin{algorithm}                      % enter the algorithm environment
\caption{Fisher-Yates-Knuth-Durstenfeld shuffle (streaming version)}          % give the algorithm a caption
\label{FYKD-streaming}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]               % enter the algorithmic environment
\Let{$i$}{0}
\Let{$a$}{[]}
\While{there are records left}
    \Let{$i$}{$i+1$}
    \Let{$J$}{random integer uniformly distributed on $\{1, \dots, i\}$}
    \If{$J < i$}
        \Let{$a[i]$}{$a[J]$}
        \Let{$a[J]$}{next record}
    \Else
        \Let{$a[i]$}{next record} 
    \EndIf
\EndWhile \\
\Return{$a$}
\end{algorithmic}
\end{algorithm}


\begin{proof}[Proof that the streaming shuffle works]
We prove by induction. The base case $i=1$ is trivial.
At stage $i$, $i>1$, suppose that all $(i-1)!$ permutations of $\{1, 2, \dots, i-1\}$ are equally likely.
$J$ may take on $i-1$ distinct values less than $i$, and the swapping procedure thus yields $(i-1)! \times (i-1)$
possible distinct permutations.
If $J = i$, then there is no swap and $(i-1)$ possible distinct permutations.
In total, there are $(i-1)! \times (i-1 + 1) = i!$ possible distinct permutations, and all are equally likely because
all values of $J$ and permutations of the first $(i-1)$ elements were equally likely.
\end{proof}


\subsubsection{\texttt{Random\_Sample}}
This algorithm is attributed to \citet{cormen_introduction_2009}.
It is a recursive algorithm that requires only $k$ random integers and does not require sorting.
\todo{prove by recursion that the method works}


\begin{algorithm}                      % enter the algorithm environment
\caption{$Random\_Sample$}
\label{Random_Sample}
\begin{algorithmic}[1]               % enter the algorithmic environment
\Require{$n \geq k \geq 0$}
\Statex
\Function{Random\_Sample}{$n, k$}
\If{$k$ is $0$}
    \Return{the empty set}
\Else
     \Let{$S$}{\texttt{Random\_Sample}($n-1, k-1$)}
     \Let{$i$}{random integer uniformly distributed on $\{1, \dots, n\}$} 
     \If{$i$ is in $S$}
           \Let{$S$}{$S \cup \{n\}$}
     \Else
            \Let{$S$}{$S\cup\{i\}$}  
     \EndIf, 
     \Return{$S$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Instances of problems}
The sampling algorithms in the previous section rely on the ability to generate random integers uniformly distributed 
on various ranges.
In this section, we illustrate several known failures in common software packages.
 

\subsubsection{Linear Congruential Generators and RANDU}

Linear congruential generators (LCGs) have the form $X_{n+1} = (a X_n + c) \mod m$, for a modulus $m$, 
multiplier $a$, and an additive parameter $c$.
The behavior of LCGs is well-understood from fundamental number theory.
There are three criteria used to choose good values of the parameters of a LCG:
the LCG should generate the full period of length $m$,
the full period sequence $X_1, \dots, X_{m-1}$ should be statistically indistinguishable from random,
and the multiplication and modulus operations should be able to be implemented efficiently in bitwise arithmetic (\cite{hornfeck_multiplicative_2009}).
For instance, there is theory on which LCGs yield a full period. \todo{cite theorem}

\begin{theorem}[Hull-Dobell Full Period Theorem]
\label{thm:hull_dobell_period}
The period of an LCG is $m$ for all seeds $X_0$ if and only if
\begin{itemize}
\item $m$ and $c$ are relatively prime,
\item $a-1$ is divisible by all prime factors of $m$, and
\item $a-1$ is divisible by 4 if $m$ is divisible by 4.
\end{itemize}
\end{theorem}

For the sake of computational speed, programmers developed LCGs using moduli of the form $m = 2^b$, where
$b$ was the integer word size of the computer.
Such an LCG violates the first principle of choosing good parameters, as no non-prime $m$ can yield an LCG with a full period.


\citet{marsaglia_random_1968} proved that $n$-tuples of numbers generated by any LCG will lie on parallel hyperplanes, making them especially non-random.
This phenomenon occurs regardless of the choice of parameters $a, c$ and $m$, but there are particularly bad choices.
One particularly nefarious version of this LCG is RANDU, a PRNG promulgated in the 1960s and widely copied. 
RANDU takes the form

$$X_{n+1} = 65539 X_n \mod 2^{31}.$$

It has a relatively short period ($2^{29}$), all its outputs are odd integers, and it fails the spectral test, which studies the lattice structures of LCGs.
Triples of values from RANDU fall on 15 planes in 3-dimensional space.
\todo{mention RANDU blowing up science, finding erroneous crystallographic structure}

Better LCGs have been developed, but they are generally considered to be insufficiently random for use in statistics.
\todo{mention and cite some, say Super-Duper}

\subsubsection{\citet{wichmann_algorithm_1982} and Excel}
The Wichmann-Hill PRNG is a sum of three LCGs, used to produce random values on $[0, 1)$.
Given three seed values $s_1, s_2, s_3$, the next value $r$ is given by
\begin{align*}
s_1 &= 171 s_1\mod(30269) \\
s_2 &= 172 s_2 \mod(30307) \\
s_3 &= 170 s_3 \mod(30323) \\
r &= (s_1/30269 + s_2/30307 + s_3/30323) \mod(1)
\end{align*}

The Wichmann-Hill generator is generally not considered adequate for statistics, but was (nominally) the PRNG in Excel for several generations. 
Excel did not allow the seed to be set, so analyses were not reproducible.
Moreover, the generator in Excel had an implementation bug that persisted for several generations.
Excel didn't allow the seed to be set so issues could not be replicated, but users reportedly generated negative numbers on occasion (\cite{mccullough_microsoft_2008}).

\subsubsection{Stata software}
Prior to April 2011, the \texttt{rnormal()} function in Stata 10 exhibited predictable behavior:
95.1\% of the $2^{31}$ values that could be used to seed the PRNG resulted in first and second draws having the same sign.
Resetting the seed too frequently, therefore, would cause this behavior to happen more frequently than if the 
random values came from a true normal distribution.
This is a case of ``burn-in'': some PRNGs require many advances in the state space before exhibiting ``random'' behavior.
Resetting the seed frequently does not allow the PRNG this burn-in period.
Stata uses the KISS algorithm to generate PRNs.
KISS is a combination of an LCG with three other PRNGs, each with a different period.
 \todo{describe KISS and cite Marsaglia}
Resetting the seed in Stata updates the seed of the LCG, but not the other three PRNGs, so KISS will exhibit the same
``numbers fall in the planes'' phenomenon as RANDU and other LCGs (\cite{ozier_perils_2012}).


\subsubsection{Random integer generation in R}
The sampling and permutation algorithms described above require $k$ or $n$ independent random integers
uniformly distributed on $\{1, \dots, m\}$ for varying values of $m$, depending on the algorithm.
A standard way to generate a random integer is to start with $X \sim U[0,1)$ and define $Y \equiv 1 + \lfloor mX \rfloor$. 
In theory, that's fine. 
But in practice, $X$ is not really $U[0,1)$ but instead is derived by normalizing a PRN that's uniform on $w$-bit integers. 
Then, unless $m$ is a power of 2, the distribution of $Y$ isn't uniform on $\{1, \ldots, m\}$. 

\begin{lemma}
For $m < 2^w$, the ratio of the largest to smallest selection probability is, to first order,  $1+ m 2^{-w}$. (See, e.g., Knuth v2 3.4.1.A.)
\end{lemma}

\begin{proof}
Define $\tilde{X}$ to be a uniform random integer on $\{0, 1, \dots, 2^w - 1\}$.
The selection probability for a particular integer value is 

\begin{align*}
\pr\left(Y = y\right) &= \pr\left(1 + \lfloor mX \rfloor = y\right) \\
&= \pr\left(y-1 \leq mX < y\right) \\
&= \pr\left(\tilde{X} < \frac{y2^w}{m}\right) - \pr\left(\tilde{X} \leq \frac{(-1)y2^w}{m}\right)\\
&= \pr\left(\tilde{X} < \left\lfloor\frac{y2^w}{m}\right\rfloor\right) - \pr\left(\tilde{X} \leq \left\lfloor\frac{(y-1)2^w}{m}\right\rfloor\right)\\
&= 2^{-w}\left(k^+(y-1)- k^-(y-1)\right)
\end{align*}

\noindent where, for fixed $m$, we define $k^-(i) \equiv \min \{k: k2^{-w} \geq i/m\}$ for all $i$,
$k^+(i) \equiv \max \{k : k2^{-w} < i/m \} = k^-(i+1)-1$ for $i = 0, \dots, m-1$
and $k^+(m) \equiv 2^w$.
The maximum ratio of selection probabilities is 

\begin{align*}
\max_{i, j \in \{0, \ldots, m-1\}} \frac{k^+(i) - k^-(i)}{k^+(j) - k^-(j)}
&= \frac{ \max_{i=0}^{m-1} (k^+(i) - k^-(i))}{\min_{i=0}^{m-1} (k^+(i) - k^-(i))} \\
&= \frac{ \max_{i=0}^{m-1} (k^+(i) - k^+(i+1) + 1)}{\min_{i=0}^{m-1} (k^-(i+1) - k^-(i) - 1)} \\
&= \frac{\lceil 2^w/m \rceil + 1}{\lfloor 2^w/m \rfloor -1}.
\end{align*}
\end{proof}
\todo{is this proof right? seems wrong}

For $m = 10^9$ and $w=32$, $1 + m 2^{-w} \approx 1.233$. That could easily matter.
In R, one would generally use the function \texttt{sample(1:m, k, replace=FALSE)} to draw pseudo-random integers. 
It seems that \texttt{sample()} uses the faulty $1 + \lfloor mX\rfloor$ approach.
\todo{illustrate with a figure}

A better way to generate a (pseudo-)random integer on $\{1, \ldots m\}$ from a (pseudo-random) $w$-bit integer in practice is as follows:
\begin{enumerate}
\item Set $\mu = \log_2(m-1)$.
\item Define a $w$-bit mask consisting of $\mu$ bits set to 1 and $(w-\mu)$ bits set to zero.
\item Generate a random $w$-bit integer $Y$.
\item Define $y$ to be the bitwise and of $y$ and the mask.
\item If $y \le m-1$, output $x = y+1$; otherwise, return to step 3.
\end{enumerate}
This is how random integers are generated in \texttt{numpy} by \texttt{numpy.random.randint()}. 
However, \texttt{numpy.random.choice()} does something else that's biased: it finds the closest integer to $mX$.

\subsection{The right way to generate PRNs}
\subsubsection{Mersenne Twister}
\subsubsection{Methods based on cryptographic hash functions}

\section{Results}
\subsection{Possibility Bounds using the Pidgeon-Hole Principle}
We now consider whether, in principle, a particular PRNG combined with a particular sampling algorithm could generate an SRS of size $k$ from a population of size $n$.
We also consider whether a particular PRNG combined with an ``optimal'' sampling algorithm that minimized the number of random bits required to generate samples, rather than ``wasting'' random bits, could generate an SRS of size $k$ from a population of size $n$.


\begin{lemma}[One output per state]
If an algorithm uses at least one (entire) output of a PRNG, each state of the PRNG produces at most one distinct output of the algorithm.
\end{lemma}

For instance, an algorithm for drawing a sample might ``consume'' more than one state of the PRNG, but each initial state of the PRNG yields at most one sample.

\begin{corollary}
The number of distinct permutations of a set of $n$ items attainable by assigning a PRN to each element and sorting the result is less than or equal to the number of states of the PRNG.
\end{corollary}

In particular, Theorem~\ref{thm:hull_dobell_period} implies that an LCG with modulus $m$ can generate at most $m$ permutations.


\begin{corollary}
The number of distinct samples of size $k$ of a set of $n$ items attainable by a method that uses at least one PRN state to select the sample is less than or equal to the number of states of the PRNG.
\end{corollary}

\begin{proposition}
The algorithm that constructs permutations of a set of $n$ objects by assigning a PRN to each, then sorting, cannot construct all permutations of a set of $n$ objects if $n! > S$, where $S$ is the number of states of the PRNG. 
In particular, using a PRNG with a 32-bit state space, such an algorithm cannot construct all permutations of a set of 13 or more objects.
Using a PRNG with a 128-bit state space, such an algorithm cannot construct all permutations of a set of 35 or more objects.
Using the Mersenne Twister, such an algorithm cannot construct all permutations of a set of 2081 or more objects.
\end{proposition}


\begin{proof}
For a 32-bit PRNG,
$$13! = 6,227,020,800 > 2^{32} = 4,294,967,296 > 12! = 479,001,600.$$
For a 128-bit PRNG,
$$ 35! = 1.03331479664 \times 10^{40} > 2^{128} \approx 3.402e+38 > 34! = 2.63130836934 \times 10^{35}. $$
For the Mersenne Twister, which has $2^{32 \times 623}$ states, \todo{I think the following line is wrong?}
 $$ \ln (2081!) \ge \ln\left( \sqrt{2 \pi} 2081^{2081.5} e^{-2081} \right) \ge 13823.83 > 13818.582 = \log(32 \times 623),$$
 $$ \log_2(2081 !) \ge \log_2\left( \sqrt{2 \pi} 2081^{2081.5} e^{-2081} \right) \ge 19943.6 > 19936 = 32 \times 623,$$
where the first inequality follows from Stirling's bound\footnote{
Stirling's bound for factorials is $ \sqrt{2 \pi} n^{n+1/2} e^{-n} \le n! \le e n^{n+1/2} e^{-n}$.}.
\end{proof}

Therefore, the usual proof that PIKK gives an SRS cannot apply for $n \ge 13$ if the PRNG has a 32-bit state space, nor for $n \ge 35$ if the PRNG has a 128-bit state space, nor for $n \ge 2081$ for the Mersenne Twister.
To show that PIKK works for larger $n$ would require showing the frequencies with which the permutations corresponding to distinct samples of size $k$ each occur are equal, despite the fact that not all permutations occur.
However, if the PRNG can attain a total of $2^w$ states, then unless $2^w$ is divisible by ${n \choose k}$, it's impossible that there are an equal number of permutations corresponding to each sample of size $k$ from $n$. 
This is the usual situation since ${n \choose k }$ is generally not a power of 2.



\begin{proposition}
Any algorithm that constructs a subset of $k$ of $n$ objects by using at least one PRNG state per sample cannot construct all $n \choose k$ subsets if the number of states of the PRNG is less than $n \choose k$.
\end{proposition}

In particular, no PRNG with 32-bit or smaller state space can construct all samples of size $10$ from a population of size $47$ or more. 
No PRNG with 128-bit or smaller state space can construct all samples of size $25$ from a population of size $366$ or more. 
The Mersenne Twister cannot construct all samples of size 1000 from a population of size $3.8 \times 10^8$ or more.

\begin{proof}
Use raw calculation for PRNGs with 32-bit and 128-bit state and entropy bounds for the Mersenne Twister.

For a 32-bit PRNG,
$${47\choose10} = 5,178,066,751>  2^{32} = 4,294,967,296 .$$
For a 128-bit PRNG,
$${366\choose25} \approx 3.406 \times 10^{38} \ge 2^{128} \approx 3.402 \times 10^{38}.$$
For the Mersenne Twister,
define $H(q) \equiv -q\log_2(q) - (1-q)\log_2(1-q)$.

$${3.8 \times 10^8 \choose 1000} \geq \frac{2^{ 3.8 \times 10^8 H(3.8 \times 10^{-5})}}{1 + 3.8 \times 10^8}$$

\todo{flesh out}
\end{proof}

$3.8 \times 10^8$ is big, but not in the world of big data.

\subsection{Theoretical bias}

Suppose we take simple random samples of size $k$ from a population of size $n$.
Denote the population by $\mathbf{X} = \{X_1, \dots, X_n\}$ and define the set of all possible samples by
$\Omega = \{\omega : \omega \subset \mathbf{X}, \lvert \omega \rvert = k \}$.
Let $F$ be the distribution over samples: that is, $F(\omega) = {n \choose k}^{-1}$ for all $\omega \in \Omega$.
Define $\tilde{F}$ to be the empirical distribution of samples over a period of the PRNG.

Suppose the PRNG never hits a subset $S \subseteq \Omega$.
Mathematically, 
$$S \equiv \left\lbrace \omega : \tilde{F}(\omega) = 0, F(\omega) = {n \choose k}^{-1}\right\rbrace.$$
Furthermore, the PRNG may not generate the other possible samples on $S^c$ with equal probability.
In practice, we know that a PRNG can only hit a certain \textit{number} of samples, but we don't know \textit{which ones}.
\todo{For example, a PRNG with a 32-bit state space can generate no more than $2^{32}$ possible subsets of $k$ of $n$ objects, 
so for $k = 10$ and $n = 47,$ $\nu \geq { 47 \choose 10} - 2^{32} = 883,099,455$.
}
If our particular PRNG omits $\nu$ samples, then define
$$\mathcal{S} = \left\lbrace S : S \subseteq \Omega, \lvert S \rvert \geq \nu\right\rbrace.$$
$\mathcal{S}$ contains all possible sets of samples that the PRNG could miss.

Define $\mathcal{G} = \{ G: G(S) = 0, S \in \mathcal{S}\}$ to be the set of all distributions on $\Omega$ with at least $\nu$ samples with 0 probability.
By definition, $\tilde{F} \in \mathcal{G}$.
Conceptually, the distribution functions in $\mathcal{G}$ take the mass $F$ assigns to subsets $S$ and 
redistributes it to other samples.
This leads to a simple lower bound on the distance between $F$ and distributions in $\mathcal{G}$.

\begin{lemma}
For any $G \in \mathcal{G}$, $ \lVert F - G \rVert_1 \geq \frac{2\nu}{{n \choose k}}$
\end{lemma}

\begin{proof}
Fix $S \in \mathcal{S}$ and choose some $G \in \mathcal{G}$ such that $G(S) = 0$.
\begin{align*}
\lVert F - G \rVert_1 &= \sum_{\omega \in \Omega} \lvert F(\omega) - G(\omega) \rvert \\
&= \sum_{\omega \in S} \lvert F(\omega) - G(\omega) \rvert + \sum_{\omega \in S^c} \lvert F(\omega) - G(\omega) \rvert\\
&= \sum_{\omega \in S} \lvert F(\omega) \rvert + \sum_{\omega \in S^c} \lvert F(\omega) - G(\omega) \rvert\\
&= \frac{ \lvert S \rvert}{{n \choose k}}+ \sum_{\omega \in S^c} \lvert F(\omega) - G(\omega) \rvert\\
&\geq \frac{ \lvert S \rvert}{{n \choose k}}+ \left\lvert\sum_{\omega \in S^c} \left( F(\omega) - G(\omega) \right)\right\rvert\\
&= \frac{ \lvert S \rvert}{{n \choose k}}+ \left\lvert\sum_{\omega \in S^c} F(\omega) - 1 \right\rvert\\
&= \frac{2 \lvert S \rvert}{{n \choose k}}
\end{align*}

Finally, $ \lVert F - G \rVert_1 \geq \inf_{S \in \mathcal{S}} \frac{2 \lvert S \rvert}{{n \choose k}} \geq \frac{2\nu}{{n \choose k}}$.
\end{proof}

\begin{proof}[another way]
Fix $S$ and choose $G \in \mathcal{G}$ such that $G(S) = 0, G(\omega) > 0$ for $\omega \in S^c$.
\begin{align*}
\lVert F - G \rVert_1 &= \sum_{\omega \in \Omega} \lvert F(\omega) - G(\omega) \rvert \\
&= \sum_{\omega \in S} \lvert F(\omega) - G(\omega) \rvert + \sum_{\omega \in S^c} \lvert F(\omega) - G(\omega) \rvert\\
&= \sum_{\omega \in S} \lvert F(\omega) \rvert + \sum_{\omega \in S^c} \lvert F(\omega) - G(\omega) \rvert\\
&= \frac{ \lvert S \rvert}{{n \choose k}}+ \sum_{\omega \in S^c} \lvert F(\omega) - (F(\omega) + \eps_\omega) \rvert\\
\end{align*}

\noindent where $\eps_\omega \in [ - {n \choose k}^{-1}, 1 - {n\choose k}^{-1}]$ and $\sum_{\omega \in S^c} \eps_\omega = \sum_{\omega \in S} F(\omega) =  \frac{ \lvert S \rvert}{{n \choose k}}$.
NB this must be the case to ensure that $\sum_{\omega} G(\omega) = 1$, since
$$\sum_{\omega} G(\omega) =\sum_{\omega\in S^c} G(\omega) = \sum_{\omega\in S^c} F(\omega) + \eps_\omega = \sum_{\omega\in S^c} F(\omega) + \sum_{\omega\in S} F(\omega) = 1.$$

Therefore,
\begin{align*}
\lVert F - G \rVert_1 &= \frac{ \lvert S \rvert}{{n \choose k}}+ \sum_{\omega \in S^c} \lvert \eps_\omega \rvert\\
&=  \frac{ \lvert S \rvert}{{n \choose k}}+ \sum_{\omega \in S} \lvert F(\omega) \\
&= \frac{2 \lvert S \rvert}{{n \choose k}}
\end{align*}
\end{proof}

\todo{relate this back to test statistics, e.g. the mean
Then for any bounded function $\psi: \Omega \to \reals$ and for any $G \in \mathcal{G}$,

$$\left\lvert \int \psi dG - \int \psi dF \right\rvert \leq \lVert F - G \rVert_1 \lVert \psi \rVert_\infty$$
}

\subsection{Simulations}
\section{Discussion}
\subsection{Best Practices}
\section{Conclusions}

\bibliographystyle{plainnat}
\bibliography{refs}




\end{document}
